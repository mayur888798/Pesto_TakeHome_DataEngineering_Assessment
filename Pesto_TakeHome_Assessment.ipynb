{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGwfAeDjgAWE"
   },
   "source": [
    "# Data Engineering Case Study\n",
    "---------------\n",
    "\n",
    "Imagine you are a data engineer working for AdvertiseX, a digital advertising technology company. AdvertiseX specializes in programmatic advertising and manages multiple online advertising campaigns for its clients. The company handles vast amounts of data generated by ad impressions, clicks, conversions, and more. Your role as a data engineer is to address the following challenges:\n",
    "\n",
    "- ### Data Sources and Formats:\n",
    "\n",
    "  - #### Ad Impressions:\n",
    "\n",
    "    **Data Source:** AdvertiseX serves digital ads to various online platforms and websites. \\\n",
    "    **Data Format:** Ad impressions data is generated in JSON format, containing information such as ad creative ID, user ID, timestamp, and the website where the ad was displayed.\n",
    "\n",
    "  - #### Clicks and Conversions:\n",
    "\n",
    "    **Data Source:** AdvertiseX tracks user interactions with ads, including clicks and conversions (e.g., sign-ups, purchases). \\\n",
    "    **Data Format:** Click and conversion data is logged in CSV format and includes event timestamps, user IDs, ad campaign IDs, and conversion type.\n",
    "\n",
    "  - #### Bid Requests:\n",
    "\n",
    "    **Data Source:** AdvertiseX participates in real-time bidding (RTB) auctions to serve ads to users. \\\n",
    "    **Data Format:** Bid request data is received in a semi-structured format, mostly in Avro, and includes user information, auction details, and ad targeting criteria.\n",
    "\n",
    "- ### Case Study Requirements:\n",
    "\n",
    "  - #### Data Ingestion:\n",
    "\n",
    "    Implement a scalable data ingestion system capable of collecting and processing ad impressions (JSON), clicks/conversions (CSV), and bid requests (Avro) data. \\\n",
    "    Ensure that the ingestion system can handle high data volumes generated in real-time and batch modes.\n",
    "\n",
    "  - #### Data Processing:\n",
    "\n",
    "    Develop data transformation processes to standardize and enrich the data. Handle data validation, filtering, and deduplication. \\\n",
    "    Implement logic to correlate ad impressions with clicks and conversions to provide meaningful insights.\n",
    "\n",
    "  - #### Data Storage and Query Performance:\n",
    "\n",
    "    Select an appropriate data storage solution for storing processed data efficiently, enabling fast querying for campaign performance analysis. \\\n",
    "    Optimize the storage system for analytical queries and aggregations of ad campaign data.\n",
    "\n",
    "  - #### Error Handling and Monitoring:\n",
    "\n",
    "    Create an error handling and monitoring system to detect data anomalies, discrepancies, or delays. \\\n",
    "    Implement alerting mechanisms to address data quality issues in real-time, ensuring that discrepancies are resolved promptly to maintain ad campaign effectiveness.\n",
    "\n",
    "This Ad Tech case study scenario focuses on the challenges and data formats commonly encountered in the digital advertising industry. Candidates can use this information to design a data engineering solution that addresses the specific data processing and analysis needs of AdvertiseX.\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "---------------\n",
    "\n",
    "There are few ways to address the challenges mentioned above. However, based on different cloud platforms like GCP, AWS, etc. or Big data Platform (Hadoop), these service names may vary.   \n",
    "\n",
    "- ### Letâ€™s break down each requirement:\n",
    "\n",
    "    - #### Data Ingestion\n",
    "        - Scalable Data Ingestion System:\n",
    "            - Set up a robust data ingestion pipeline capable of handling high volumes of real-time and batch data.\n",
    "                - Google Pub/Sub, Apache Kafka or AWS Kinesis can be used for real-time streaming data ingestion.\n",
    "                - Use Apache Airflow for periodic batch data loads.\n",
    "        - Data Source-Specific Adapters:\n",
    "            - Develop custom adapters for each data source (ad impressions, clicks/conversions, bid requests) which can handle data retrieval, format conversion, and initial validation.\n",
    "            \n",
    "                We can use `json` library to parse json and `avro` library to parse avro file. However, pandas provide a high level abstraction to load them as dataframe as such "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastavro import reader\n",
    "from google.cloud import bigquery\n",
    "from pymongo import MongoClient\n",
    "from google.cloud import storage\n",
    "\n",
    "# Data Ingestion\n",
    "\n",
    "def ingest_ad_impressions(json_file_path):\n",
    "    \"\"\"\n",
    "    Ingests ad impressions data from a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path (str): Path to the JSON file containing ad impressions data.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        ad_impressions_data = pd.read_json(json_file)\n",
    "        # Process and store ad impressions data (e.g., insert into a database)\n",
    "        # Example: Insert into MongoDB or a relational database\n",
    "        # ...\n",
    "\n",
    "# Usage\n",
    "json_file_path = 'path/to/ad_impressions.json'\n",
    "ingest_ad_impressions(json_file_path)\n",
    "\n",
    "def ingest_clicks_and_conversions(csv_file_path):\n",
    "    \"\"\"\n",
    "    Ingests clicks and conversions data from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path (str): Path to the CSV file containing clicks and conversions data.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    click_conversion_df = pd.read_csv(csv_file_path)\n",
    "    # Process and store click/conversion data (e.g., filter, validate, enrich)\n",
    "    # Example: Validate timestamps, join with campaign details\n",
    "    # ...\n",
    "\n",
    "# Usage\n",
    "csv_file_path = 'path/to/clicks_conversions.csv'\n",
    "ingest_clicks_and_conversions(csv_file_path)\n",
    "\n",
    "def ingest_bid_requests(avro_file_path):\n",
    "    \"\"\"\n",
    "    Ingests bid requests data from an Avro file.\n",
    "    \n",
    "    Args:\n",
    "        avro_file_path (str): Path to the Avro file containing bid requests data.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(avro_file_path, 'rb') as avro_file:\n",
    "        avro_df = pd.DataFrame(reader(avro_file))\n",
    "        # Process and store bid request data (e.g., extract user info, auction details)\n",
    "        # Example: Extract user demographics, validate auction details\n",
    "        # ...\n",
    "\n",
    "# Usage\n",
    "avro_file_path = 'path/to/bid_requests.avro'\n",
    "ingest_bid_requests(avro_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Data Processing\n",
    "    - Standardization and Enrichment:\n",
    "        - Transform raw data into a common schema for consistency.\n",
    "            - Convert timestamps to a consistent format (e.g., ISO 8601)\n",
    "            - Handle missing values (e.g., fill or drop)\n",
    "            - Standardize column names (e.g., rename columns)\n",
    "        - Enrich data by adding relevant metadata (e.g., campaign details, user demographics, Top of Funnel (TOF), Bottom of Funnel (BOF), conversion ratio, ).\n",
    "        - Use tools like Apache Spark or Databricks for scalable data processing.\n",
    "    - Correlation Logic:\n",
    "        - Join ad impressions with clicks and conversions based on common identifiers (e.g., user ID, creative ID).\n",
    "        - Calculate click-through rates (CTR) and conversion rates.\n",
    "        - Identify successful campaigns and optimize targeting strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "\n",
    "def correlate_impressions_with_clicks(impressions_df, clicks_df):\n",
    "    \"\"\"\n",
    "    Correlates ad impressions with clicks.\n",
    "    \n",
    "    Args:\n",
    "        impressions_df (pd.DataFrame): DataFrame containing ad impressions data.\n",
    "        clicks_df (pd.DataFrame): DataFrame containing clicks data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with correlated data.\n",
    "    \"\"\"\n",
    "    # Assuming both DataFrames have common identifiers (e.g., user ID, creative ID)\n",
    "    # Merge ad impressions with clicks\n",
    "    correlated_df = impressions_df.merge(clicks_df, on='user_id', how='left')\n",
    "    return correlated_df\n",
    "\n",
    "# Usage\n",
    "correlated_data_df = correlate_impressions_with_clicks(ad_impressions_df, clicks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Data Storage and Query Performance\n",
    "    - Storage Solution Selection:\n",
    "        - Choose an appropriate storage system based on requirements:\n",
    "            - **Data Warehouse:** For structured data (e.g., clicks/conversions), use solutions like Amazon Redshift, Google BigQuery, or Snowflake. Below is sample code to query with Bigquery with python client library.\n",
    "\n",
    "    - Optimization for Analytical Queries\n",
    "    \n",
    "        - Create materialized views in BigQuery to pre-aggregate data for common queries (e.g., daily campaign performance).\n",
    "        - Define the view using SQL and schedule refresh intervals.\n",
    "        - Partition tables by hourly, daily, monthly, yearly (e.g., daily or hourly partitions) to improve query efficiency.\n",
    "        - Use Clustering for field which have categorical data. \n",
    "        - Use ingestion-time partitioning for streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Storage and Query Performance\n",
    "\n",
    "def store_ad_impressions(dataframe):\n",
    "    \"\"\"\n",
    "    Stores ad impressions data into MongoDB.\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame containing ad impressions data.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    db = client['advertising_db']\n",
    "    collection = db['ad_impressions']\n",
    "\n",
    "    # Insert data into MongoDB\n",
    "    collection.insert_many(dataframe.to_dict('records'))\n",
    "\n",
    "    # Close the connection\n",
    "    client.close()\n",
    "\n",
    "# Usage\n",
    "store_ad_impressions(ad_impressions_df)\n",
    "\n",
    "def upload_to_gcs(local_file_path, bucket_name, object_name):\n",
    "    \"\"\"\n",
    "    Uploads a file to Google Cloud Storage (GCS).\n",
    "    \n",
    "    Args:\n",
    "        local_file_path (str): Path to the local file to upload.\n",
    "        bucket_name (str): Name of the GCS bucket.\n",
    "        object_name (str): Name of the object to create in GCS.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(object_name)\n",
    "\n",
    "    # Upload the local file to GCS\n",
    "    blob.upload_from_filename(local_file_path)\n",
    "\n",
    "# Usage\n",
    "local_file_path = 'path/to/local_file.avro'\n",
    "bucket_name = 'your-gcs-bucket'\n",
    "object_name = 'ad_impressions.avro'\n",
    "upload_to_gcs(local_file_path, bucket_name, object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Error Handling and Monitoring\n",
    "\n",
    "    - Set up monitoring tools (e.g., Grafana, or Google Cloud Monitoring) and create data quality dashboard to track data quality metrics (e.g., data completeness, latency).\n",
    "    - Monitor data pipelines and identify bottlenecks or failures.\n",
    "    - Implement automated retries for failed data ingestion or processing tasks.\n",
    "    - Detect anomalies (e.g., sudden drops in impressions, high CTR) using statistical methods.\n",
    "    - Trigger alerts via email, Slack, or SMS when discrepancies occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case Study Requirements: Error Handling and Monitoring\n",
    "\n",
    "def monitor_data_quality():\n",
    "    \"\"\"\n",
    "    Monitors data quality metrics and triggers alerts for anomalies.\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Implement monitoring logic to detect anomalies, discrepancies, or delays\n",
    "    # Trigger alerts via email, Slack, or SMS when discrepancies occur\n",
    "    # Example: Monitor sudden drops in impressions, high CTR, etc.\n",
    "    pass\n",
    "\n",
    "# Usage\n",
    "monitor_data_quality()\n",
    "\n",
    "def handle_errors():\n",
    "    \"\"\"\n",
    "    Handles errors and exceptions in the data pipeline.\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Implement error handling mechanisms to address data anomalies, discrepancies, or delays\n",
    "    # Example: Implement automated retries for failed data ingestion or processing tasks\n",
    "    pass\n",
    "\n",
    "# Usage\n",
    "handle_errors()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
